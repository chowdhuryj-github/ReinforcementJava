{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#235347'> Introduction to Q-Learning </font>\n",
    "Q-learning is a type of reinforcement learning where an agent learns the best actions to take in different situations by trying things out and getting rewards or penalties. Over time, it builds a table (Q-table) that tells it the best move to make in any situation to maximize its total rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#235347'> Initializing the Q-Table & Grid World </font> \n",
    "We start by creating a simple 3x3 grid world. The agent will start at the top-left corner, and the goal is at the bottom-right corner. The agent can move in four directions: up, down, left and right.\n",
    "\n",
    "The Q-table is like a agent's memory. It keeps track of how good each action is at each position in the grid. \n",
    "- For each state (position) in the grid, the agent stores a Q-value for each possible action\n",
    "- The Q-value represents how good it is to take a certain action from a certain state\n",
    "\n",
    "The agent can be at any 9 positions on the grid. At each position, the agent can choose from 4 actions, Up, Down, Left and Right. The Q-table will be a 3D table with the following dimensions:\n",
    "- 3 (rows) x 3 (columns) x 4 (actions) which gives us a 3x3x4 table\n",
    "\n",
    "When we initialize a Q-table at the start, we initialize it with zeros because the agent doesn't know anything. This means that the agent thinks all actions are good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the necessary imports\n",
    "import numpy as np\n",
    "\n",
    "# defining the size of the grid\n",
    "grid_size = 3\n",
    "\n",
    "# the reward function\n",
    "def reward(state):\n",
    "    if state == (2, 2):\n",
    "        print(\"Goal Reached!\")\n",
    "        return 1 \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# creating a 3x3x4 q-table\n",
    "q_table = np.zeros((3, 3, 4))\n",
    "\n",
    "# printing out the q-table\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#235347'> Defining the Agent's Movements </font> \n",
    "In this step, we define how the agent moves in a 3x3 grid based on its chosen action. The agent will be able to move up, down, left or right, but we'll also make sure it doesn't move outside the bounds of the grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the movement function\n",
    "def move(state, action):\n",
    "\n",
    "    # retrieving the current position\n",
    "    x, y = state\n",
    "\n",
    "    # the actions\n",
    "    if action == \"up\" and x > 0:\n",
    "        return (x-1, y)\n",
    "    if action == \"down\" and x < grid_size - 1:\n",
    "        return (x+1, y)\n",
    "    if action == \"left\" and y > 0:\n",
    "        return (x, y-1)\n",
    "    if action == \"right\" and y < grid_size - 1:\n",
    "        return (x, y+1)\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#235347'> Exploration vs Exploitation </font> \n",
    "In Q-learning, the agent needs to decide which action to take at each state. It does by balancing between two things:\n",
    "- Exploration: trying random actions to discover the environment\n",
    "- Exploitation: choosing the best known action based on the Q-values\n",
    "\n",
    "In order to achieve this, we use a **epsilon-greedy** strategy. \n",
    "- With probability epsilon, the agent will explore (choose a random action)\n",
    "- With probability 1 - epsilon, the agent will exploit (choose action with highest Q-values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# epsilon-greedy action selection function\n",
    "def choose_action(state, epsilon=0.2):\n",
    "\n",
    "    # generates a random floating point number\n",
    "    # 0.2 means there is a 20% change the agent will explore\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        # exploration: choose a random action\n",
    "        return random.choice(['up', 'down', 'left', 'right'])\n",
    "    \n",
    "    else:\n",
    "        # exploitation: choose the action with the q-value\n",
    "        # q_table[state_x, state_y] accesses the q-values for the current state\n",
    "        # np.argmax() finds the index of the action with the highest Q-value in that array\n",
    "        state_x, state_y = state\n",
    "        return ['up', 'down', 'left', 'right'][np.argmax(q_table[state_x, state_y])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#235347'> Updating the Q-table </font>\n",
    "Next, we work on updating the Q-table after the agent takes an action. The goal of Q-learning is for the agent to improve its decision making over time by learning from the environment. This is done by updating the Q-values in the Q-table based on the agent's experience. This is the formula we use to update the Q-values:\n",
    "\n",
    "### <font color='#235347'> Defining the Q-learning parameters </font>\n",
    "We now define the following Q-learning parameters:\n",
    "1. Learning Rate: how much the agent should learn from new experiences\n",
    "2. Discount Factor: how much the agent should care about future rewards\n",
    "3. Reward Function: the reward the agent gets after taking an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the learning rate\n",
    "alpha = 0.1\n",
    "\n",
    "# defining the discount factor\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#235347'> Writing the Update Rule for Q-table </font>\n",
    "Using the parameters that have been set-up, we now implement the Q-learning update rule. This is the formula we use to update the Q-values:\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left( r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \\right)\n",
    "$$\n",
    "\n",
    "Here are the following explanations behind each of the parameters:\n",
    "- $ Q(s_t, a_t) $ is the current Q-value for the state-action pair. This is in the Q-value\n",
    "- $ R(s,a) $ is the immediate reward the agent receives after taking action \"a\" in state \"s\"\n",
    "- $ \\gamma \\max_{a'} Q(s', a') $ is the estimated future reward, after the agent takes an action, it transitions to the next state \"s' \"\n",
    "- We then update the current Q-value with the new information from the formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to update q-value\n",
    "def update_q_value(state, action, reward, next_state):\n",
    "    state_x, state_y = state\n",
    "    next_state_x, next_state_y = next_state\n",
    "\n",
    "    # finding the action index\n",
    "    action_index = [\"up\", \"down\", \"left\", \"right\"].index(action)\n",
    "\n",
    "    # find the max q-value for the next state\n",
    "    max_future_q = np.max(q_table[next_state_x, next_state_y])\n",
    "\n",
    "    # applying the q-learning update rule\n",
    "    current_q = q_table[state_x, state_y, action_index]\n",
    "    q_table[state_x, state_y, action_index] += alpha * (reward + gamma * max_future_q - current_q)\n",
    "\n",
    "    # debugging print statement\n",
    "    print(f\"Updated Q-value for state {state}, action {action}: {q_table[state_x, state_y, action_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#235347'> Simulating a Episode & Updating Q-table </font>\n",
    "Finally, we will now simulate the agent starting at a random position on the grid and updating the Q-table. The goal of this is to allow the agent to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulating an episode\n",
    "def run_episode(start_state=(0,0), max_steps=10):\n",
    "\n",
    "    # the starting state\n",
    "    state = start_state\n",
    "\n",
    "    # tracking the total reward for the episode\n",
    "    total_reward = 0\n",
    "\n",
    "    # running the episode a fixed number of steps\n",
    "    for _ in range(max_steps):\n",
    "\n",
    "        action = choose_action(state)\n",
    "        next_state = move(state, action)\n",
    "        reward_value = reward(next_state)\n",
    "        total_reward += reward_value\n",
    "        update_q_value(state, action, reward_value, next_state)\n",
    "        state = next_state\n",
    "\n",
    "        # goal checking \n",
    "        if state == (2, 2):\n",
    "            print(f\"Goal reached at step {_+1}!\")\n",
    "            break\n",
    "    \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action left: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action left: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action left: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action left: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action left: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action right: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action left: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action right: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action down: 0.0\n",
      "Updated Q-value for state (1, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action down: 0.0\n",
      "Updated Q-value for state (1, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action down: 0.0\n",
      "Updated Q-value for state (1, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action right: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action down: 0.0\n",
      "Updated Q-value for state (1, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action down: 0.0\n",
      "Updated Q-value for state (1, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action left: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action down: 0.0\n",
      "Updated Q-value for state (1, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action left: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action right: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action left: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action down: 0.0\n",
      "Updated Q-value for state (1, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action right: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action down: 0.0\n",
      "Updated Q-value for state (1, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action down: 0.0\n",
      "Updated Q-value for state (1, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action right: 0.0\n",
      "Updated Q-value for state (0, 2), action down: 0.0\n",
      "Updated Q-value for state (1, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action right: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action down: 0.0\n",
      "Updated Q-value for state (1, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action left: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action left: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action down: 0.0\n",
      "Updated Q-value for state (1, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action down: 0.0\n",
      "Updated Q-value for state (1, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action right: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action left: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action down: 0.0\n",
      "Updated Q-value for state (1, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action left: 0.0\n",
      "Updated Q-value for state (0, 0), action right: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action down: 0.0\n",
      "Updated Q-value for state (1, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action right: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action left: 0.0\n",
      "Updated Q-value for state (0, 1), action right: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action right: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action left: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action right: 0.0\n",
      "Updated Q-value for state (0, 2), action down: 0.0\n",
      "Updated Q-value for state (1, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action left: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action right: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action down: 0.0\n",
      "Updated Q-value for state (1, 2), action right: 0.0\n",
      "Updated Q-value for state (1, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action left: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action right: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action right: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action right: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action right: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action left: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action down: 0.0\n",
      "Updated Q-value for state (1, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action down: 0.0\n",
      "Updated Q-value for state (1, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action down: 0.0\n",
      "Updated Q-value for state (1, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action down: 0.0\n",
      "Updated Q-value for state (1, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action right: 0.0\n",
      "Updated Q-value for state (0, 2), action down: 0.0\n",
      "Updated Q-value for state (1, 2), action left: 0.0\n",
      "Updated Q-value for state (1, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action left: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action left: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action left: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action left: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action up: 0.0\n",
      "Updated Q-value for state (0, 0), action right: 0.0\n",
      "Updated Q-value for state (0, 1), action right: 0.0\n",
      "Updated Q-value for state (0, 2), action down: 0.0\n",
      "Updated Q-value for state (1, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Updated Q-value for state (0, 2), action left: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action up: 0.0\n",
      "Updated Q-value for state (0, 1), action right: 0.0\n",
      "Updated Q-value for state (0, 2), action up: 0.0\n",
      "Total reward for this episode: 0\n",
      "Updated Q-table:\n",
      "[[[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "total_reward = run_episode(start_state=(0, 0), max_steps=1000)\n",
    "print(f\"Total reward for this episode: {total_reward}\")\n",
    "print(\"Updated Q-table:\")\n",
    "print(q_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
